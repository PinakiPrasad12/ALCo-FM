{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_test = pd.read_csv('/fs/ess/PAS0536/ppgneogi/TrafficEvents/US_Accidents_March23.csv')\n",
    "zipcode_df_1 = pd.read_csv('US_ZipCodes_30140.csv')\n",
    "zipcode_df_2 = pd.read_csv('US_ZipCodes_23021_151.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both the zipcode dataframes (demographics) together\n",
    "combined_zipcode_df = pd.concat([zipcode_df_1, zipcode_df_2], ignore_index=True)\n",
    "combined_zipcode_df = combined_zipcode_df.drop_duplicates(subset=['zip_code'])\n",
    "combined_zipcode_df = combined_zipcode_df.rename(columns={'zip_code': 'Zipcode'})\n",
    "combined_zipcode_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'Zipcode' column in df_test by removing the '-XXXX' part\n",
    "df_test['Zipcode'] = df_test['Zipcode'].str.split('-').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure both columns are of the same type (string)\n",
    "combined_zipcode_df['Zipcode'] = combined_zipcode_df['Zipcode'].astype(str)\n",
    "# Perform the merge\n",
    "merged_df = df_test.merge(combined_zipcode_df, on='Zipcode', how='left')\n",
    "    \n",
    "# Print the shape of the merged DataFrame\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to exclude\n",
    "columns_to_exclude = [\n",
    "    'ID', 'Source', 'End_Time', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n",
    "    'Street', 'County', 'Country', 'Timezone', 'Airport_Code', 'Wind_Chill(F)',\n",
    "    'Precipitation(in)', 'Weather_Timestamp', 'Temperature(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "    'Visibility(mi)', 'Wind_Direction', 'Wind_Speed(mph)', 'Weather_Condition', \n",
    "    'Unnamed: 0', 'County', 'state', 'county', 'time_zone', 'area_code', 'latitude', \n",
    "    'longitude', 'population_2019', 'population_2020', 'average_household_income_2019($)', \n",
    "    'average_household_income_2020($)', 'population_2005', 'population_2006', \n",
    "    'population_2007', 'population_2008', 'population_2009', 'population_2010', \n",
    "    'population_2011', 'population_2012', 'population_2013', 'population_2014', \n",
    "    'population_2015', 'population_2016', 'population_2017', 'population_2018', \n",
    "    'owner_occupied_home_values_from_$1_to_$24999_dollars', \n",
    "    'owner_occupied_home_values_from_$25000_to_$49999_dollars', \n",
    "    'owner_occupied_home_values_from_$50000_to_$99999_dollars', \n",
    "    'owner_occupied_home_values_from_$100000_to_$149999_dollars', \n",
    "    'owner_occupied_home_values_from_$150000_to_$199999_dollars', \n",
    "    'owner_occupied_home_values_from_$200000_to_$399999_dollars', \n",
    "    'owner_occupied_home_values_from_$400000_to_$749999_dollars', \n",
    "    'owner_occupied_home_values_more_than_$750000_dollars', \n",
    "    'rented_housing_number_of_rooms_studio', \n",
    "    'rented_housing_number_of_rooms_1_bedroom', \n",
    "    'rented_housing_number_of_rooms_2_bedroom', \n",
    "    'rented_housing_number_of_rooms_3_or_more_bedroom', \n",
    "    'average_household_income_2005($)', 'average_household_income_2006($)', \n",
    "    'average_household_income_2007($)', 'average_household_income_2008($)', \n",
    "    'average_household_income_2009($)', 'average_household_income_2010($)', \n",
    "    'average_household_income_2011($)', 'average_household_income_2012($)', \n",
    "    'average_household_income_2013($)', 'average_household_income_2014($)', \n",
    "    'average_household_income_2015($)', 'average_household_income_2016($)', \n",
    "    'average_household_income_2017($)', 'average_household_income_2018($)', \n",
    "    'household_income_less_than_25000_dollars', \n",
    "    'household_income_from_45000_to_59999_dollars', \n",
    "    'household_income_from_60000_to_99999_dollars', \n",
    "    'household_income_from_100000_to_149999_dollars', \n",
    "    'household_income_from_150000_to_199999_dollars', \n",
    "    'household_income_more_than_200000_dollars', \n",
    "    'annual_individual_earnings_less_than_10000_dollars', \n",
    "    'annual_individual_earnings_from_10000_to_19999_dollars', \n",
    "    'annual_individual_earnings_from_20000_to_29999_dollars', \n",
    "    'annual_individual_earnings_from_30000_to_39999_dollars', \n",
    "    'annual_individual_earnings_from_40000_to_49999_dollars', \n",
    "    'annual_individual_earnings_from_50000_to_64999_dollars', \n",
    "    'annual_individual_earnings_from_65000_to_74999_dollars', \n",
    "    'annual_individual_earnings_from_75000_to_99999_dollars', \n",
    "    'annual_individual_earnings_more_than_100000_dollars', \n",
    "    'earnings_source_fulltime_with_earnings', \n",
    "    'earnings_source_parttime_with_earnings', \n",
    "    'earnings_source_no_earnings', 'Sunrise_Sunset', 'Civil_Twilight', \n",
    "    'Nautical_Twilight', 'Astronomical_Twilight'\n",
    "]\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "filtered_df = merged_df.drop(columns=columns_to_exclude)\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(filtered_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by (City, State)\n",
    "grouped = filtered_df.groupby(['City', 'State'])\n",
    "\n",
    "# Calculate total records per group\n",
    "total_records = grouped.size().rename('Total_Records')\n",
    "\n",
    "# Filter for groups with more than 50,000 records\n",
    "filtered_groups = total_records[total_records > 50000].index\n",
    "\n",
    "# Subset the DataFrame for valid groups only\n",
    "filtered_df_subset = filtered_df[filtered_df.set_index(['City', 'State']).index.isin(filtered_groups)]\n",
    "\n",
    "# Recalculate missing values and align with filtered groups\n",
    "total_missing = (\n",
    "    filtered_df_subset.groupby(['City', 'State'])\n",
    "    .apply(lambda group: group.isna().sum().sum())\n",
    "    .rename('Total_Missing')\n",
    ")\n",
    "\n",
    "# Recalculate total records for filtered groups\n",
    "total_records = total_records.loc[filtered_groups]\n",
    "\n",
    "# Calculate missing ratio\n",
    "missing_ratio = (total_missing / total_records).rename('Missing_Ratio')\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "result = pd.concat([total_records, total_missing, missing_ratio], axis=1)\n",
    "\n",
    "# Sort by missing ratio\n",
    "result = result.sort_values(by='Missing_Ratio')\n",
    "\n",
    "# Display the result\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities_states = result.index[:15]\n",
    "print(top_cities_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter filtered_df to include only these (City, State) pairs\n",
    "filtered_df_top = filtered_df[filtered_df.set_index(['City', 'State']).index.isin(top_cities_states)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### H3 ID ############################\n",
    "import h3\n",
    "\n",
    "# Function to create H3 IDs based on latitude, longitude, and resolution level\n",
    "def create_h3_ids(row, level):\n",
    "    lat, lng = row['Start_Lat'], row['Start_Lng']\n",
    "    h3_id = h3.geo_to_h3(lat, lng, level)\n",
    "    return h3_id\n",
    "\n",
    "# Create the new DataFrame with the required columns for H3 resolution level 7\n",
    "new_df_7 = pd.DataFrame()\n",
    "new_df_7['H3 ID Level 7'] = filtered_df_top.apply(lambda row: create_h3_ids(row, 7), axis=1)  # Generate H3 IDs\n",
    "\n",
    "# Remove duplicate H3 ID Level 7 entries\n",
    "new_df_7 = new_df_7.drop_duplicates(subset=['H3 ID Level 7'])\n",
    "\n",
    "# Reset the index to create unique row numbers for Area ID 7\n",
    "new_df_7.reset_index(drop=True, inplace=True)\n",
    "new_df_7.reset_index(inplace=True)\n",
    "\n",
    "# Rename the index column to 'Area ID 7'\n",
    "new_df_7.rename(columns={'index': 'Area ID 7'}, inplace=True)\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "new_df_7.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"********************* Total Zipcodes present in the top 15 cities: \" + str(len(set(filtered_df_top['Zipcode']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'H3 ID Level 7' and the 'Area ID 7' for each record (based on the latitude and longitude)\n",
    "filtered_df_top['H3 ID Level 7'] = filtered_df_top.apply(lambda row: create_h3_ids(row, 7), axis=1)\n",
    "filtered_df_top = filtered_df_top.merge(new_df_7[['H3 ID Level 7', 'Area ID 7']], on='H3 ID Level 7', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count records with no missing values per 'Area ID 7'\n",
    "non_missing_counts = (\n",
    "    filtered_df_top.dropna()\n",
    "    .groupby('Area ID 7')\n",
    "    .size()\n",
    "    .reset_index(name='Non_Missing_Records')\n",
    ")\n",
    "\n",
    "# Count total records per 'Area ID 7'\n",
    "total_counts = (\n",
    "    filtered_df_top.groupby('Area ID 7')\n",
    "    .size()\n",
    "    .reset_index(name='Total_Records')\n",
    ")\n",
    "\n",
    "# Merge the two results to get both counts side-by-side\n",
    "area_counts = pd.merge(non_missing_counts, total_counts, on='Area ID 7', how='right')\n",
    "\n",
    "# Fill missing Non_Missing_Records with 0 (if an Area ID has no completely non-missing rows)\n",
    "area_counts['Non_Missing_Records'] = area_counts['Non_Missing_Records'].fillna(0).astype(int)\n",
    "\n",
    "# Add City and State information (if applicable to Area ID 7)\n",
    "city_state_mapping = filtered_df_top[['Area ID 7', 'City', 'State']].drop_duplicates()\n",
    "area_counts = pd.merge(area_counts, city_state_mapping, on='Area ID 7', how='left')\n",
    "\n",
    "# Reorder columns for better readability\n",
    "area_counts = area_counts[['Area ID 7', 'City', 'State', 'Non_Missing_Records', 'Total_Records']]\n",
    "\n",
    "# Display the final result\n",
    "print(area_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_counts_under50 = area_counts[area_counts['Total_Records'] >= 100]\n",
    "area_counts_under50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_counts_under50.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_counts_before = area_counts.groupby('City').size().reset_index(name='Total_AreIDs_before_deletion')\n",
    "print(city_counts_before)\n",
    "city_counts = area_counts_under50.groupby('City').size().reset_index(name='Total_AreIDs_after_deletion')\n",
    "print(city_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_counts_under50['Non_Missing_Ratio'] = area_counts_under50['Non_Missing_Records'] / area_counts_under50['Total_Records']\n",
    "sorted_area_counts = area_counts_under50.sort_values(by=['City', 'Non_Missing_Ratio'], ascending=[True, False])\n",
    "print(sorted_area_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = sorted_area_counts[sorted_area_counts['Non_Missing_Ratio'] >= 0.95]\n",
    "city_counts = filtered_data.groupby('City').size().reset_index(name='Total_AreIDs_after_deleting_below_95')\n",
    "print(city_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the Rows from filtered_df_top whose Area ID is present in filtered_data\n",
    "area_id_7_list = filtered_data['Area ID 7'].unique()\n",
    "filtered_df_top = filtered_df_top[filtered_df_top['Area ID 7'].isin(area_id_7_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the time range\n",
    "#start_time = pd.Timestamp('2016-03-22 00:00:00')\n",
    "start_time = pd.Timestamp('2016-06-01 00:00:00')  # Adjust start time as needed\n",
    "end_time = pd.Timestamp('2023-03-31 23:59:59')\n",
    "time_bins = pd.date_range(start=start_time, end=end_time, freq='3H')\n",
    "\n",
    "# Ensure 'Start_Time' is a datetime type\n",
    "filtered_df_top['Start_Time'] = pd.to_datetime(filtered_df_top['Start_Time'])\n",
    "\n",
    "# Pre-bin the 'Start_Time' into intervals\n",
    "filtered_df_top['time_bin'] = filtered_df_top['Start_Time'].dt.floor('3H')\n",
    "# Optional: Calculate time_bin_end if needed for further analysis\n",
    "# filtered_df_top['time_bin_end'] = filtered_df_top['time_bin'] + pd.Timedelta(hours=12)\n",
    "\n",
    "# Group by 'Area ID 7' and 'time_bin' to calculate the required aggregates\n",
    "grouped = filtered_df_top.groupby(['Area ID 7', 'time_bin']).agg(\n",
    "    Total_Accidents=('Severity', 'size'),\n",
    "    Acc_Severity=('Severity', lambda x: list(x.unique()))\n",
    ").reset_index()\n",
    "\n",
    "# Display the grouped data\n",
    "print(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all combinations of 'Area ID 7' and 'time_bins'\n",
    "all_combinations = pd.MultiIndex.from_product(\n",
    "    [filtered_df_top['Area ID 7'].unique(), time_bins], names=['Area ID 7', 'time_bin']\n",
    ")\n",
    "time_series_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "\n",
    "# Merge the grouped results back onto the time_series_df\n",
    "time_series_df = time_series_df.merge(grouped, on=['Area ID 7', 'time_bin'], how='left')\n",
    "\n",
    "# Display the time series DataFrame\n",
    "print(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values for missing time bins\n",
    "time_series_df['Total_Accidents'] = time_series_df['Total_Accidents'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN in 'Acc_Severity' with [0] for proper handling\n",
    "time_series_df['Acc_Severity'] = time_series_df['Acc_Severity'].apply(\n",
    "    lambda x: [0] if isinstance(x, float) and pd.isna(x) else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in filtered_df_top.columns:\n",
    "    if column not in ['Start_Time', 'Severity', 'Area ID 7', 'time_bin']:\n",
    "        time_series_df[column] = time_series_df['Area ID 7'].map(\n",
    "            filtered_df_top.groupby('Area ID 7')[column].first()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.to_csv('/fs/ess/PAS0536/ppgneogi/TrafficEvents/TimeSeries_WithoutWeather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#time_series_df = pd.read_csv('TimeSeries_WithoutWeather2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the file path and column selection\n",
    "file_path = \"\"  # Update with the correct path\n",
    "columns_to_select = [\n",
    "    'valid', 'tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', \n",
    "    'alti', 'vsby', 'skyc1'\n",
    "]\n",
    "start_time = pd.Timestamp('2016-06-01 00:00:00')\n",
    "end_time = pd.Timestamp('2023-03-31 23:59:59')\n",
    "\n",
    "# List of files to process\n",
    "files = [\n",
    "    \"SanDiegoWeather_imputed\", \"SacramentoWeather_imputed\", \"LosAngelesWeather_imputed\", \n",
    "    \"BatonRougeWeather_imputed\", \"OrlandoWeather_imputed\", \"MiamiWeather_imputed\", \n",
    "    \"NashvilleWeather_imputed\", \"MinneapolisWeather_imputed\", \"CharlotteWeather_imputed\", \n",
    "    \"RaleighWeather_imputed\", \"PhoenixWeather_imputed\", \"HoustonWeather_imputed\", \n",
    "    \"DallasWeather_imputed\", \"AustinWeather_imputed\", \"AtlantaWeather_imputed\"\n",
    "]\n",
    "\n",
    "# Dictionary to hold the dataframes\n",
    "weather_dataframes = {}\n",
    "\n",
    "for file_name in files:\n",
    "    # Load the data\n",
    "    file = os.path.join(file_path, file_name + \".xlsx\")\n",
    "    df = pd.read_excel(file)\n",
    "\n",
    "    # Select relevant columns\n",
    "    df = df[columns_to_select]\n",
    "\n",
    "    # Convert 'valid' column to datetime\n",
    "    df['valid'] = pd.to_datetime(df['valid'])\n",
    "\n",
    "    # Filter data within the specified time range\n",
    "    df = df[(df['valid'] >= start_time) & (df['valid'] <= end_time)]\n",
    "\n",
    "    # Create 3-hour time bins\n",
    "    df['time_bin'] = (df['valid'] - start_time).dt.total_seconds() // (3 * 3600)\n",
    "    df['time_bin'] = df['time_bin'].astype(int)\n",
    "\n",
    "    # Group by time_bin and aggregate\n",
    "    aggregated_df = df.groupby('time_bin').agg(\n",
    "        tmpf=('tmpf', 'mean'),\n",
    "        dwpf=('dwpf', 'mean'),\n",
    "        relh=('relh', 'mean'),\n",
    "        drct=('drct', 'mean'),\n",
    "        sknt=('sknt', 'mean'),\n",
    "        p01i=('p01i', 'mean'),\n",
    "        alti=('alti', 'mean'),\n",
    "        vsby=('vsby', 'mean'),\n",
    "        skyc1=('skyc1', 'last')  # Take the last value for 'skyc1'\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add the start time of the time bin\n",
    "    aggregated_df['time_bin_start'] = aggregated_df['time_bin'].apply(\n",
    "        lambda x: start_time + pd.Timedelta(hours=3 * x)\n",
    "    )\n",
    "\n",
    "    # Drop the time_bin column and reorder\n",
    "    aggregated_df = aggregated_df.drop(columns=['time_bin'])\n",
    "    aggregated_df = aggregated_df[['time_bin_start'] + \n",
    "                                   [col for col in aggregated_df.columns if col != 'time_bin_start']]\n",
    "\n",
    "    # Save the dataframe in the dictionary\n",
    "    weather_dataframes[file_name] = aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the old keys to the proper city names\n",
    "city_name_mapping = {\n",
    "    \"SanDiegoWeather_imputed\": \"San Diego\",\n",
    "    \"SacramentoWeather_imputed\": \"Sacramento\",\n",
    "    \"LosAngelesWeather_imputed\": \"Los Angeles\",\n",
    "    \"BatonRougeWeather_imputed\": \"Baton Rouge\",\n",
    "    \"OrlandoWeather_imputed\": \"Orlando\",\n",
    "    \"MiamiWeather_imputed\": \"Miami\",\n",
    "    \"NashvilleWeather_imputed\": \"Nashville\",\n",
    "    \"MinneapolisWeather_imputed\": \"Minneapolis\",\n",
    "    \"CharlotteWeather_imputed\": \"Charlotte\",\n",
    "    \"RaleighWeather_imputed\": \"Raleigh\",\n",
    "    \"PhoenixWeather_imputed\": \"Phoenix\",\n",
    "    \"HoustonWeather_imputed\": \"Houston\",\n",
    "    \"DallasWeather_imputed\": \"Dallas\",\n",
    "    \"AustinWeather_imputed\": \"Austin\",\n",
    "    \"AtlantaWeather_imputed\": \"Atlanta\"\n",
    "}\n",
    "\n",
    "# Renaming the keys in the dictionary\n",
    "weather_dataframes = {\n",
    "    city_name_mapping[key]: value for key, value in weather_dataframes.items()\n",
    "}\n",
    "\n",
    "# Display the updated keys\n",
    "print(weather_dataframes.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataframes['San Diego'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataframes['San Diego'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, df in weather_dataframes.items():\n",
    "    # Convert 'time_bin_start' to string and rename it to 'time_bin'\n",
    "    df.rename(columns={'time_bin_start': 'time_bin'}, inplace=True)\n",
    "    df['time_bin'] = df['time_bin'].astype(str)\n",
    "    df['City'] = city  # Add City column\n",
    "    # Update the dictionary with the modified DataFrame\n",
    "    weather_dataframes[city] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataframes['San Diego'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dataframes['San Diego']['time_bin'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather_df = pd.concat(weather_dataframes.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df['time_bin'] = time_series_df['time_bin'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df['time_bin'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df = pd.merge(time_series_df, combined_weather_df, on=['City', 'time_bin'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "final_merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TrafficEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
